{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **📄 Image Classification 대회**\n",
    "> 문서 타입 분류 대회\n",
    "> EfficientNet 모델을 로드하여, 모델을 학습 및 예측 파일 생성하는 프로세스\n",
    "\n",
    "## Contents\n",
    "- Imort Library & Define Functions\n",
    "- Hyper-tuning\n",
    "- Load Data\n",
    "- Train Model\n",
    "- Inderence & save File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* 학습 및 추론에 필요한 라이브러리를 로드\n",
    "* 학습 및 추론에 필요한 함수와 클래스를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from timm.data.mixup import Mixup\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과의 재현성을 위해 시드를 고정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic=True \n",
    "torch.backends.cudnn.benchmark=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 모니터링 (wandb 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgimjeongheon38\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login\n",
    "\n",
    "#entitiy : 고정 , project : '본인이름_cv' 이런 느낌, name : \"첫번쨰 시도\" ... etc  \n",
    "run = wandb.init(project=\"kimjeongheon_cv\", entity = 'CV_대회', name ='TopOfTheWorld')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b4'\n",
    "\n",
    "# training config\n",
    "img_size = 380\n",
    "LR = 0.001717\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "num_workers = 8\n",
    "\n",
    "#focal loss\n",
    "loss_alpha = 0.55\n",
    "loss_gamma = 1.477\n",
    "\n",
    "#AdamW - optimizer\n",
    "dropout_ratio = 0.3 \n",
    "\n",
    "# 증강 데이터 배수 설정\n",
    "num_augmentations = 10\n",
    "#aug\n",
    "alpha = 0.621\n",
    "mixup_prob =  0.3853\n",
    "\n",
    "use_amp=True\n",
    "\n",
    "#gradient\n",
    "accumulation_steps = 2\n",
    "\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model_name\": model_name,\n",
    "    \"img_size\": img_size,\n",
    "    \"LR\" : LR,\n",
    "    \"BATCH_SIZE\":BATCH_SIZE,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"loss_alpha\" : loss_alpha,\n",
    "    \"loss_gamma\" : loss_gamma,\n",
    "    \"dropout_ratio\" : dropout_ratio,\n",
    "    \"num_augmentations\": num_augmentations,\n",
    "    \"alpha\": alpha,\n",
    "    \"mixup_prob\":mixup_prob\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, val_loader=None, epoch=None, mixup_fn=None, accumulation_steps=2, use_amp=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    scaler = GradScaler(enabled=use_amp)  # GradScaler 초기화\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "\n",
    "    for i, (image, targets) in enumerate(pbar):\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        if mixup_fn is not None:\n",
    "            image, targets = mixup_fn(image, targets)\n",
    "\n",
    "        with autocast(enabled=use_amp):  # autocast 컨텍스트 매니저 사용\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        loss = loss / accumulation_steps  # 손실 정규화\n",
    "\n",
    "        scaler.scale(loss).backward()  # 스케일링된 그래디언트 계산\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            if use_amp:\n",
    "                scaler.step(optimizer)  # 스케일링된 그래디언트로 옵티마이저 업데이트\n",
    "                scaler.update()  # 스케일러 업데이트\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.argmax(dim=1).detach().cpu().numpy())  # 정수 레이블로 변환\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    if val_loader:\n",
    "        val_preds, val_targets = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "                if mixup_fn is not None:\n",
    "                    images, targets = mixup_fn(images, targets)\n",
    "\n",
    "                preds = model(images)\n",
    "                val_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        val_loss = loss_fn(preds, targets).item()\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "\n",
    "        ret.update({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"val_preds\": val_preds,  # val_preds를 결과 딕셔너리에 추가\n",
    "            \"val_targets\": val_targets,  # val_targets를 결과 딕셔너리에 추가\n",
    "        })\n",
    "        \n",
    "        if epoch is not None:\n",
    "                    epoch_interval = max(1, EPOCHS // 20)  # 전체 epochs의 1/20 간격으로 로그 출력\n",
    "                    if (epoch + 1) % epoch_interval == 0 or epoch == 0 or epoch == EPOCHS - 1:\n",
    "                        print(f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                            f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, Resize, HorizontalFlip, VerticalFlip,\n",
    "    RandomRotate90, Rotate, GaussianBlur, HueSaturationValue,\n",
    "    RandomBrightnessContrast, Normalize\n",
    ")\n",
    "\n",
    "# 증강 기법 정의\n",
    "def get_train_augmentation(img_size, mixup_prob=0.5, alpha=1.0):\n",
    "    augmentation = Compose([\n",
    "        RandomResizedCrop(height=img_size, width=img_size, scale=(0.8, 1.0), p=0.5),\n",
    "        Resize(height=img_size, width=img_size),\n",
    "        HorizontalFlip(p=0.6),\n",
    "        VerticalFlip(p=0.6),\n",
    "        RandomRotate90(p=0.5),\n",
    "        Rotate(limit=(-35, 35), p=0.5),\n",
    "        GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        \n",
    "        # 클래스 간 차이 부각을 위한 추가 기법\n",
    "        A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),  # 이미지 압축 및 품질 저하\n",
    "        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.5), # CoarseDropout 추가\n",
    "        Normalize(mean=[0.57433558, 0.58330406, 0.58818927],\n",
    "                    std=[0.18964056, 0.18694252, 0.18506919]),\n",
    "                    #직접 구함\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Wandb에 증강 기법 기록\n",
    "    wandb.config.update({\"augmentation\": str(augmentation)})\n",
    "    return augmentation\n",
    "\n",
    "# 테스트 데이터 변환 기법 정의\n",
    "def get_test_augmentation(img_size):\n",
    "    return Compose([\n",
    "        Resize(height=img_size, width=img_size),\n",
    "        Normalize(mean=[0.57433558, 0.58330406, 0.58818927],\n",
    "                    std=[0.18964056, 0.18694252, 0.18506919]),\n",
    "                    #직접 구함\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, num_augmentations=1):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_augmentations = num_augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) * self.num_augmentations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.num_augmentations\n",
    "        image_path = self.image_paths[image_idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[image_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            if 'mixup' in augmented:\n",
    "                label = augmented['mixup']['target']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"조기 종료(Early stopping) 을 위한 클래스\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_f1_max = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_f1, model):\n",
    "        score = val_f1\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_f1, model):\n",
    "        \"\"\"모델의 매개변수를 저장합니다\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation F1 score increased ({self.val_f1_max:.6f} --> {val_f1:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_f1_max = val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return focal_loss\n",
    "        elif self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "train_df = pd.read_csv(\"/data/ephemeral/home/data/train.csv\", usecols=['ID', 'target'])\n",
    "train_image_paths = [f\"/data/ephemeral/home/data/train/{fname}\" for fname in train_df['ID']]\n",
    "train_labels = train_df['target'].values\n",
    "\n",
    "test_df = pd.read_csv(\"/data/ephemeral/home/data/sample_submission.csv\")\n",
    "test_image_paths = test_df['ID'].apply(lambda x: f\"/data/ephemeral/home/data/test/{x}\").tolist()\n",
    "test_labels = [0] * len(test_image_paths)  # 테스트 데이터셋에는 레이블이 없으므로 더미 레이블 사용\n",
    "\n",
    "tst_dataset = ImageDataset(test_image_paths, test_labels, transform=get_test_augmentation(img_size))\n",
    "tst_loader = DataLoader(tst_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0553: 100%|██████████| 785/785 [01:16<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.2346, Val Loss: 0.1185, Train Acc: 0.8455, Val Acc: 0.8736, Train F1: 0.8365, Val F1: 0.8507\n",
      "Validation F1 score increased (inf --> 0.850680).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0125: 100%|██████████| 785/785 [01:17<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score increased (0.850680 --> 0.877809).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 785/785 [01:18<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score increased (0.877809 --> 0.898740).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2785: 100%|██████████| 785/785 [01:18<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0044: 100%|██████████| 785/785 [01:17<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100, Train Loss: 0.1074, Val Loss: 0.0146, Train Acc: 0.9305, Val Acc: 0.8854, Train F1: 0.9287, Val F1: 0.8784\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0021: 100%|██████████| 785/785 [01:18<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1973: 100%|██████████| 785/785 [01:17<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 score increased (0.898740 --> 0.899823).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0083: 100%|██████████| 785/785 [01:18<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 785/785 [01:17<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022: 100%|██████████| 785/785 [01:18<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100, Train Loss: 0.0848, Val Loss: 0.1421, Train Acc: 0.9409, Val Acc: 0.9194, Train F1: 0.9402, Val F1: 0.9177\n",
      "Validation F1 score increased (0.899823 --> 0.917740).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 785/785 [01:17<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004: 100%|██████████| 785/785 [01:39<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 785/785 [02:28<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 785/785 [02:26<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 8.5850e-04.\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0367: 100%|██████████| 785/785 [02:28<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100, Train Loss: 0.0698, Val Loss: 1.5060, Train Acc: 0.9469, Val Acc: 0.9115, Train F1: 0.9468, Val F1: 0.9098\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:31<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0172: 100%|██████████| 785/785 [02:27<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.0657, Val Loss: 0.0006, Train Acc: 0.9505, Val Acc: 0.9726, Train F1: 0.9494, Val F1: 0.9710\n",
      "Validation F1 score increased (0.917740 --> 0.970975).  Saving model ...\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:34<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 785/785 [02:29<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.0656, Val Loss: 0.0002, Train Acc: 0.9449, Val Acc: 0.9675, Train F1: 0.9443, Val F1: 0.9693\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:33<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 785/785 [02:31<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.0597, Val Loss: 0.0000, Train Acc: 0.9505, Val Acc: 0.9758, Train F1: 0.9496, Val F1: 0.9756\n",
      "Validation F1 score increased (0.970975 --> 0.975580).  Saving model ...\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:31<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 785/785 [02:33<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 0.0605, Val Loss: 0.0001, Train Acc: 0.9531, Val Acc: 0.9739, Train F1: 0.9522, Val F1: 0.9734\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [00:32<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stratified K-Fold Cross Validation 설정\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "wandb.config.update({\"n_splits\": str(n_splits)})\n",
    "\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "# 5개의 Fold에 대한 예측 확률을 저장할 리스트\n",
    "pred_probs = []\n",
    "\n",
    "# 모델 초기화\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "\n",
    "loss_fn = FocalLoss(alpha = loss_alpha, gamma=loss_gamma)\n",
    "model.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "# AdamW 옵티마이저 사용\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR,weight_decay=0.03994)\n",
    "wandb.config.update({\"optimizer\": str(optimizer)})\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# 조기 종료 설정\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "# 각 fold에 대해 모델 학습 및 평가\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_image_paths, train_labels)):\n",
    "    print(f'Fold {fold+1}/{n_splits}')\n",
    "\n",
    "    # 학습 데이터와 검증 데이터 분리\n",
    "    train_paths = [train_image_paths[i] for i in train_idx]\n",
    "    train_labels_ = [train_labels[i] for i in train_idx]\n",
    "    val_paths = [train_image_paths[i] for i in val_idx]\n",
    "    val_labels = [train_labels[i] for i in val_idx]\n",
    "\n",
    "    # 데이터셋 및 데이터로더 생성\n",
    "    train_dataset = ImageDataset(train_paths, train_labels_, transform=get_train_augmentation(img_size, mixup_prob=0.5, alpha=1.0), num_augmentations=num_augmentations)\n",
    "    val_dataset = ImageDataset(val_paths, val_labels, transform=get_train_augmentation(img_size, mixup_prob=0.5, alpha=1.0), num_augmentations=num_augmentations)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "    # 모델 학습\n",
    "    for epoch in range(EPOCHS):\n",
    "        mixup_fn = Mixup(mixup_alpha=alpha, cutmix_alpha=0.0, \n",
    "                         prob=mixup_prob, switch_prob=0.0, mode='batch', label_smoothing=0.0, num_classes=17)\n",
    "        results = train_one_epoch(train_loader, model, optimizer, loss_fn, device, val_loader, epoch, mixup_fn)\n",
    "\n",
    "        # 학습률 스케줄러 업데이트\n",
    "        scheduler.step(results['val_f1'])\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": results[\"train_loss\"], \"train_acc\": results[\"train_acc\"], \"train_f1\": results[\"train_f1\"],\n",
    "                   \"val_loss\": results[\"val_loss\"], \"val_acc\": results[\"val_acc\"], \"val_f1\": results[\"val_f1\"]})\n",
    "        \n",
    "        # 조기 종료 확인\n",
    "        early_stopping(results['val_f1'], model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # 모델 평가\n",
    "    val_preds = results['val_preds']\n",
    "    val_targets = results['val_targets']\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "    fold_scores.append(val_f1)\n",
    "    wandb.log({\"val_f1\": val_f1})\n",
    "\n",
    "    \n",
    "    # 현재 Fold 모델의 테스트 데이터에 대한 예측 확률 저장\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fold_pred_probs = []\n",
    "        for image, _ in tqdm(tst_loader):\n",
    "            image = image.to(device)\n",
    "            preds = model(image)\n",
    "            fold_pred_probs.extend(preds.softmax(dim=1).detach().cpu().numpy())\n",
    "        pred_probs.append(fold_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Macro F1 Score: 0.9598\n"
     ]
    }
   ],
   "source": [
    "# 전체 평균 점수 계산\n",
    "mean_score = np.mean(fold_scores)\n",
    "print(f'Mean Macro F1 Score: {mean_score:.4f}')\n",
    "wandb.log({\"mean_score\": mean_score})\n",
    "\n",
    "pred_probs_mean = np.mean(pred_probs, axis=0)\n",
    "preds_list_argmax = np.argmax(pred_probs_mean, axis=1)\n",
    "wandb.log({\"pred_probs\": pred_probs})\n",
    "wandb.log({\"preds_list\": preds_list_argmax})\n",
    "\n",
    "submission_df = pd.read_csv(\"/data/ephemeral/home/data/sample_submission.csv\")\n",
    "submission_df['target'] = preds_list_argmax\n",
    "submission_df.to_csv(\"submission1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\UAReggresion\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from timm.data.mixup import Mixup\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, val_loader=None, epoch=None, mixup_fn=None):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        if mixup_fn is not None:\n",
    "            image, targets = mixup_fn(image, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    if val_loader:\n",
    "        val_preds, val_targets = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "                if mixup_fn is not None:\n",
    "                    images, targets = mixup_fn(images, targets)\n",
    "\n",
    "                preds = model(images)\n",
    "                val_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        val_loss = loss_fn(preds, targets).item()\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "\n",
    "        ret.update({\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"val_preds\": val_preds,  # val_preds를 결과 딕셔너리에 추가\n",
    "            \"val_targets\": val_targets,  # val_targets를 결과 딕셔너리에 추가\n",
    "        })\n",
    "\n",
    "        if epoch is not None:\n",
    "            print(f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, Resize, HorizontalFlip, VerticalFlip,\n",
    "    RandomRotate90, Rotate, GaussianBlur, HueSaturationValue,\n",
    "    RandomBrightnessContrast, Normalize\n",
    ")\n",
    "\n",
    "# 증강 기법 정의\n",
    "def get_train_augmentation(img_size, mixup_prob=0.5, alpha=1.0):\n",
    "    return Compose([\n",
    "        RandomResizedCrop(height=img_size, width=img_size, scale=(0.8, 1.0), p=0.5),\n",
    "        Resize(height=img_size, width=img_size),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "        Rotate(limit=(-35, 35), p=0.5),\n",
    "        GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        Normalize(mean=[0.57433558, 0.58330406, 0.58818927], std=[0.18964056, 0.18694252, 0.18506919]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "# 테스트 데이터 변환 기법 정의\n",
    "def get_test_augmentation(img_size):\n",
    "    return Compose([\n",
    "        Resize(height=img_size, width=img_size),\n",
    "        Normalize(mean=[0.67112013, 0.67663422, 0.6792661], std=[0.19423191, 0.19232531, 0.19091303]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, num_augmentations=1):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_augmentations = num_augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) * self.num_augmentations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.num_augmentations\n",
    "        image_path = self.image_paths[image_idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[image_idx]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            if 'mixup' in augmented:\n",
    "                label = augmented['mixup']['target']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"조기 종료(Early stopping) 을 위한 클래스\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_f1_max = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_f1, model):\n",
    "        score = val_f1\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_f1, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_f1, model):\n",
    "        \"\"\"모델의 매개변수를 저장합니다\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation F1 score increased ({self.val_f1_max:.6f} --> {val_f1:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_f1_max = val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgimjeongheon38\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    wandb.init(project=\"kimjeongheon_cv\", entity = 'upstage-ai-comp', name ='test1')\n",
    "    \"\"\"\n",
    "    def 제외하고 복붙하면 된다는 . \n",
    "    \"\"\"\n",
    "    # 시드를 고정합니다.\n",
    "    SEED = 42\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # model config\n",
    "    model_name = 'efficientnet_b4' #'resnet34' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "    # training config\n",
    "    img_size = 384\n",
    "    LR = wandb.config.lr\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 16\n",
    "    num_workers = 0\n",
    "\n",
    "    # 증강 데이터 배수 설정\n",
    "    num_augmentations = 5\n",
    "\n",
    "    alpha = wandb.config.alpha\n",
    "    mixup_prob = wandb.config.p\n",
    "\n",
    "    train_df = pd.read_csv(\"/data/ephemeral/home/data/train.csv\", usecols=['ID', 'target'])\n",
    "    train_image_paths = [f\"/data/ephemeral/home/data/train/{fname}\" for fname in train_df['ID']]\n",
    "    train_labels = train_df['target'].values\n",
    "\n",
    "    test_df = pd.read_csv(\"/data/ephemeral/home/data/sample_submission.csv\")\n",
    "    test_image_paths = test_df['ID'].apply(lambda x: f\"/data/ephemeral/home/data/test/{x}\").tolist()\n",
    "    test_labels = [0] * len(test_image_paths)  # 테스트 데이터셋에는 레이블이 없으므로 더미 레이블 사용\n",
    "\n",
    "    tst_dataset = ImageDataset(test_image_paths, test_labels, transform=get_test_augmentation(img_size))\n",
    "    tst_loader = DataLoader(tst_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    # Stratified K-Fold Cross Validation 설정\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "\n",
    "    # 5개의 Fold에 대한 예측 확률을 저장할 리스트\n",
    "    pred_probs = []\n",
    "\n",
    "    # 각 fold에 대해 모델 학습 및 평가\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_image_paths, train_labels)):\n",
    "        print(f'Fold {fold+1}/{n_splits}')\n",
    "\n",
    "        # 학습 데이터와 검증 데이터 분리\n",
    "        train_paths = [train_image_paths[i] for i in train_idx]\n",
    "        train_labels_ = [train_labels[i] for i in train_idx]\n",
    "        val_paths = [train_image_paths[i] for i in val_idx]\n",
    "        val_labels = [train_labels[i] for i in val_idx]\n",
    "\n",
    "        # 데이터셋 및 데이터로더 생성\n",
    "        train_dataset = ImageDataset(train_paths, train_labels_, transform=get_train_augmentation(img_size, mixup_prob=0.5, alpha=1.0), num_augmentations=num_augmentations)\n",
    "        val_dataset = ImageDataset(val_paths, val_labels, transform=get_train_augmentation(img_size, mixup_prob=0.5, alpha=1.0), num_augmentations=num_augmentations)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # AdamW 옵티마이저 사용\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "        # 학습률 스케줄러 설정\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "        # 조기 종료 설정\n",
    "        early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "        # 모델 학습\n",
    "        for epoch in range(EPOCHS):\n",
    "            mixup_fn = Mixup(mixup_alpha=alpha, cutmix_alpha=0.0, prob=mixup_prob, switch_prob=0.0, mode='batch', label_smoothing=0.0, num_classes=17)\n",
    "            results = train_one_epoch(train_loader, model, optimizer, loss_fn, device, val_loader, epoch, mixup_fn)\n",
    "\n",
    "            # 학습률 스케줄러 업데이트\n",
    "            scheduler.step(results['val_f1'])\n",
    "\n",
    "            # 조기 종료 확인\n",
    "            early_stopping(results['val_f1'], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # 모델 평가\n",
    "        val_preds = results['val_preds']\n",
    "        val_targets = results['val_targets']\n",
    "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "        fold_scores.append(val_f1)\n",
    "        \n",
    "        # 현재 Fold 모델의 테스트 데이터에 대한 예측 확률 저장\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            fold_pred_probs = []\n",
    "            for image, _ in tqdm(tst_loader):\n",
    "                image = image.to(device)\n",
    "                preds = model(image)\n",
    "                fold_pred_probs.extend(preds.softmax(dim=1).detach().cpu().numpy())\n",
    "            pred_probs.append(fold_pred_probs)\n",
    "    \n",
    "    \n",
    "    # 전체 평균 점수 계산\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f'Mean Macro F1 Score: {mean_score:.4f}')\n",
    "\n",
    "    pred_probs = np.mean(pred_probs, axis=0)\n",
    "    preds_list = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "    submission_df = pd.read_csv(\"/data/ephemeral/home/data/sample_submission.csv\")\n",
    "    submission_df['target'] = preds_list\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bysi',\n",
    "    'name': 'sweep',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n",
    "    \n",
    "    'parameters': \n",
    "    {\n",
    "        'lr': {'max': 0.01, 'min': 0.0001}, # 스케 \n",
    "        'alpha' : {'max': 1.0, 'min': 0.4},\n",
    "        'p' : {'max': 0.7, 'min': 0.4},\n",
    "        # p : aug probabilities.\n",
    "        #'mix_up' : aug 기법 < 어느정도 , 지금은 0.5 \n",
    "        # 여기서 유의미한 값이 최신 모델에도 유용할까... 해봐야 앎.\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep by passing in config.\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"<project>\")\n",
    "\n",
    "# Start sweep job.\n",
    "wandb.agent(sweep_id, function=main, count=4) #main 함수를 넣어줘야되는대. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
